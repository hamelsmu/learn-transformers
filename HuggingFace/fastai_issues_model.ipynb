{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c32cfc",
   "metadata": {},
   "source": [
    "# Fine Tune Text Summarizer With Hugging Face\n",
    "\n",
    "Trying to adapt and follow: https://github.com/huggingface/notebooks/blob/master/examples/summarization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80cde3",
   "metadata": {},
   "source": [
    "This will fine tune a model to summarize GitHub Issues from the GitHub repo fastai/fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da5def2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ghapi in /opt/conda/lib/python3.7/site-packages (0.1.16)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from ghapi) (20.9)\n",
      "Requirement already satisfied: fastcore in /opt/conda/lib/python3.7/site-packages (from ghapi) (1.3.20)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from ghapi) (21.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->ghapi) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "! pip install ghapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad38913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ghapi.core import GhApi\n",
    "from ghapi.all import github_token, paged\n",
    "import os, pickle\n",
    "from fastcore.all import L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24209c2a",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1278d",
   "metadata": {},
   "source": [
    "Uncomment this block and run it if this is the first time running this notebook.  You need to have a [personal access token](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) in an environment variable named `GH_PAT`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474c28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = GhApi(owner='fastai', repo='fastai', token=os.getenv('GH_PAT'))\n",
    "# issues = L(paged(api.issues.list_for_repo, state='all')).concat()\n",
    "# pickle.dump(issues, open( \"issues.p\", \"wb\" ) )\n",
    "# len(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ca2322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "metric = load_metric(\"rouge\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c94487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(model.training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cbeb53",
   "metadata": {},
   "source": [
    "## Process The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb9d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "issues = pickle.load(open( \"issues.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1cceab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [{'body': \"Currently there is a bug where if you run `lr_find()` twice in a row, you will get very different results such as below: \\r\\n![image](https://user-images.githubusercontent.com/7831895/96666881-f7f9e800-1325-11eb-858a-9eae1ae82b12.png)\\r\\n\\r\\nThis sort of pattern is common in models that already have trained weights. Since we know that the models weights are stored away, my investigation led me to believe that something would be wrong with how we are loading in the optimizer in `learn.load`. \\r\\n\\r\\nThe stem of the issue is the fact that if `self.opt` is none we call `create_opt` and then pass this *new* opt into `load_model` as seen below:\\r\\n```python\\r\\ndef load(self, file, with_opt=None, device=None, **kwargs):\\r\\n        if device is None and hasattr(self.dls, 'device'): device = self.dls.device\\r\\n        if self.opt is None: self.create_opt()\\r\\n        file = join_path_file(file, self.path/self.model_dir, ext='.pth')\\r\\n        load_model(file, self.model, self.opt, device=device, **kwargs)\\r\\n        return self\\r\\n```\\r\\n\\r\\nWhat winds up happening is the same optimizer state is being used, despite us trying to load in new weights because `self.opt` is not `None` when it goes in. The proposed fix is that if `with_opt` is `None`, we set `self.opt` to `None`. This allows for a much better graph that we could expect:\\r\\n```python\\r\\n@patch\\r\\ndef load(self:Learner, file, with_opt=None, device=None, **kwargs):\\r\\n    if device is None and hasattr(self.dls, 'device'): device = self.dls.device\\r\\n    if with_opt is None: self.opt=None\\r\\n    file = join_path_file(file, self.path/self.model_dir, ext='.pth')\\r\\n    load_model(file, self.model, self.opt, device=device, **kwargs)\\r\\n    return self\\r\\n```\\r\\n\\r\\n![image](https://user-images.githubusercontent.com/7831895/96667077-5b841580-1326-11eb-856a-c35537a094c6.png)\\r\\n\\r\\nLet me know if this fix looks okay to you @jph00, or if you believe there is a better solution for addressing the issue \\r\\n\", 'title': 'Learn.load and LRFinder not functioning properly for the optimizer states'},{'body': 'following the [conversation](https://forums.fast.ai/t/fbeta-formula-for-averages-references-non-existing-metric/40965) on the dev forum:\\r\\n\\r\\nWhenever the precision does not go over all the classes it returns `nan`s in some of it\\'s elements, hence the end `*` and `sum()` result in the overall `nan`:\\r\\n\\r\\nhttps://github.com/fastai/fastai/blob/bc093a9de9c9d6c3d2c7e5ad6d7a427face1ca9a/fastai/metrics.py#L215\\r\\n\\r\\nThis happens in the beginning of training. Given enough epochs, and/or once `precision` saw all the classes, `fbeta` has a number to show.\\r\\n\\r\\nAdded `metric[metric != metric] = 0` before multiplying it by weights. It ensures \"`nan`\"s are converted to \"`0`\"s and the resulting fbeta is never \"`nan`\":\\r\\n\\r\\n| epoch | train_loss | valid_loss | accuracy | precision | recall | fbeta | time\\r\\n|---|---|---|---|---|---|---|---|\\r\\n| 0 | 1.847709 | 1.456625 | 0.537861 | nan | 0.309992 | **nan** | 01:15\\r\\n\\r\\nbecomes:\\r\\n\\r\\n| epoch | train_loss | valid_loss | accuracy | precision | recall | fbeta | time\\r\\n|---|---|---|---|---|---|---|---|\\r\\n| 0 | 1.847709 | 1.456625 | 0.537861 | nan | 0.309992 | **0.303910** | 01:15\\r\\n\\r\\nwith an actual number for `fbeta`', 'title': 'fix \"FBeta\" score calc on precision with \"nan\"s'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = (issues\n",
    " .filter(lambda x: x.body and x.title and len(x.body) > 10 and len(x.title) > 5)\n",
    " .map(lambda x: {'body':x.body, 'title':x.title}).shuffle()\n",
    "  )\n",
    "pairs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea31c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = pairs[:2800]\n",
    "eval_pairs = pairs[2800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a14350",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d74fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples.map(lambda x: x['body'])]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True, )\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(list(examples.map(lambda x: x['title'])), max_length=max_target_length, padding=True, truncation=True,)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89d07017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GH_Issues(Dataset):\n",
    "    def __init__(self, data): self.data = data\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {x: self.data[x][idx] for x in ['input_ids', 'attention_mask', 'labels']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d6b4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = {}\n",
    "tokenized_datasets['train'] = GH_Issues(preprocess_function(train_pairs))\n",
    "tokenized_datasets['eval'] = GH_Issues(preprocess_function(eval_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fd53f",
   "metadata": {},
   "source": [
    "## Fine Tuning The Model With Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "854a430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e735dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test-summarization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f387d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a40db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5164be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.485000</td>\n",
       "      <td>13.994021</td>\n",
       "      <td>21.195700</td>\n",
       "      <td>4.761900</td>\n",
       "      <td>17.029000</td>\n",
       "      <td>17.029000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>2.052700</td>\n",
       "      <td>1.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.784900</td>\n",
       "      <td>13.994021</td>\n",
       "      <td>21.195700</td>\n",
       "      <td>4.761900</td>\n",
       "      <td>17.029000</td>\n",
       "      <td>17.029000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.529900</td>\n",
       "      <td>1.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13.879900</td>\n",
       "      <td>13.994021</td>\n",
       "      <td>21.195700</td>\n",
       "      <td>4.761900</td>\n",
       "      <td>17.029000</td>\n",
       "      <td>17.029000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.540600</td>\n",
       "      <td>1.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13.076100</td>\n",
       "      <td>13.994021</td>\n",
       "      <td>21.195700</td>\n",
       "      <td>4.761900</td>\n",
       "      <td>17.029000</td>\n",
       "      <td>17.029000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.324400</td>\n",
       "      <td>2.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>13.554100</td>\n",
       "      <td>13.971494</td>\n",
       "      <td>21.195700</td>\n",
       "      <td>4.761900</td>\n",
       "      <td>17.029000</td>\n",
       "      <td>17.029000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.897400</td>\n",
       "      <td>1.581000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=13.355995559692383, metrics={'train_runtime': 10.6076, 'train_samples_per_second': 0.471, 'total_flos': 6120850083840.0, 'epoch': 5.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 124715008, 'train_mem_gpu_alloc_delta': 485873152, 'train_mem_cpu_peaked_delta': 68141056, 'train_mem_gpu_peaked_delta': 1459021824})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18dada16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"Hi,\n",
    "\n",
    "I have recently been looking at using fastcore.script as a way to create lightweight scripts around functions which are also imported and used elsewhere. However, I seem to be experiencing some odd behaviour, which I will illustrate with a trivial example.\n",
    "\n",
    "Letâ€™s say I define a script `say_hello_script.py` which contains:\n",
    "```\n",
    "from fastcore.script import *\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def say_hello(greeting: Param('Greeting', str) = 'Hello',\n",
    "              name: Param('Name', str) = 'World'):\n",
    "    print(f'{greeting}, {name}')\n",
    "```\n",
    "\n",
    "If I call this from the command line using `python say_hello_script.py --name Chris`, I get the expected output of `Hello, Chris.` All is good so far.\n",
    "\n",
    "However, now I want to call this function from a different script, so I will create a script `greeter.py` which contains:\n",
    "```\n",
    "from say_hello_script import say_hello\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    say_hello(greeting='hi', name='person')\n",
    "```\n",
    "\n",
    "Running `python greeter.py` I get the output of `Hello, World`, which seems to have bypassed my arguments! Is this the normal behaviour?\n",
    "\n",
    "Thanks,\n",
    "Chris\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "50e3a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text2=\"\"\"New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "... A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "... Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "... In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "... Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "... 2010 marriage license application, according to court documents.\n",
    "... Prosecutors said the marriages were part of an immigration scam.\n",
    "... On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "... After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "... Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "... All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "... Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "... Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "... The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "... Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "... Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "... If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "... \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54316b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tokenizer.encode(prefix+test_text, \n",
    "                return_tensors=\"pt\",\n",
    "                max_length=max_input_length, \n",
    "                truncation=True, \n",
    "                padding=True).to('cuda')\n",
    "\n",
    "# inp['labels'] = tokenizer('', \n",
    "#                 return_tensors=\"pt\",\n",
    "#                 max_length=max_input_length, \n",
    "#                 truncation=True, \n",
    "#                 padding=True).to('cuda').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99041ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8774, 1150, 5575, 32, 1047, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello World foobar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "417e89fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8774, 1150, 5575, 32, 1047, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello World foobar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bec0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b081ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inp, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b996ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<pad> a script <unk>say_hello_script.py<unk> contains: <unk> from fastcore.script import * @call_parse def say_hello(greeting: Param('Greeting', str) = 'Hello', name: Param('Name', str) = 'World'. if I call this from the command line using <unk>python say_hello_script.py --name Chris<unk>, I get the expected output of</s>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "52de4da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(out.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34c9ab31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'past_key_values', 'encoder_last_hidden_state'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198038c",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- We don't need to enable training before using the trainer?  Does the trainer automatically enable training mode, and then disable training mode when its done?  While browsing the code for Trainer, it appears that the attribute `is_in_train` is set to True and then later set to False at the end of training, but I am not 100% sure. \n",
    "- Is there a way to grab the recommended Training arguments for the model I'm using from the Hub rather that manually specifying them myself using `Seq2SeqTrainingArguments`?  It seems that the defaults for `Seq2SeqTrainingArguments` are not model-specific, but perhaps there is a way to get this?  \n",
    "- Similarly is there a way to automatically grab the metrics or recommended metrics to use that were trained the model just to ensure consistency from the model hub?  \n",
    "- Why is `model` passed into DataCollatorForSeq2Seq?  I can see from the code that it is using the model's `prepare_decoder_input_ids_from_labels` attribute, but isn't that something that would/should also be available in a tokenizer instead?  I'm just trying to build a better mental model of what is happening.  \n",
    "- Is there a way to create my own pipeline object similar to the high level magic you have for pretrained models?  Like is there a way I can leverage some of the same machinery you have so I don't have to create my own inference machinery?  I know I can create it myself, but wondering if there is a way to avoid having to build a thing that takes strings, tokenizes, numericalizes, does a forward pass, then decodes that back into a string with beam search etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a2c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10507361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
