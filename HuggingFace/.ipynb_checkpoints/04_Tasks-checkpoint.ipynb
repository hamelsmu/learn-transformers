{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.790879487991333}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', 'bert-base-cased-finetuned-mrpc')\n",
    "classifier('I love it here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrase Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    ">>> import torch\n",
    "\n",
    ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    ">>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-cased-finetuned-mrpc and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('fill-mask', \"bert-base-cased-finetuned-mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'Austin is the capital of answered',\n",
       "  'score': 0.0024273262824863195,\n",
       "  'token': 3845,\n",
       "  'token_str': 'answered'},\n",
       " {'sequence': 'Austin is the capital ofeking',\n",
       "  'score': 0.0020825688261538744,\n",
       "  'token': 25819,\n",
       "  'token_str': '##eking'},\n",
       " {'sequence': 'Austin is the capital ofender',\n",
       "  'score': 0.0020735478028655052,\n",
       "  'token': 15981,\n",
       "  'token_str': '##ender'},\n",
       " {'sequence': 'Austin is the capital ofÎ¾',\n",
       "  'score': 0.0018538099247962236,\n",
       "  'token': 28350,\n",
       "  'token_str': '##Î¾'},\n",
       " {'sequence': 'Austin is the capital ofzer',\n",
       "  'score': 0.0017360658384859562,\n",
       "  'token': 6198,\n",
       "  'token_str': '##zer'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Austin is the capital of [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    ">>> sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    ">>> sequence_1 = \"Apples are especially bad for your health\"\n",
    ">>> sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    ">>> not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> paraphrase_classification_logits = model(**paraphrase).logits\n",
    ">>> not_paraphrase_classification_logits = model(**not_paraphrase).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    ">>> not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 50%\n",
      "is paraphrase: 50%\n",
      "not paraphrase: 50%\n",
      "is paraphrase: 50%\n"
     ]
    }
   ],
   "source": [
    ">>> for i in range(len(classes)):\n",
    "...     print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "\n",
    "\n",
    ">>> # Should not be paraphrase\n",
    ">>> for i in range(len(classes)):\n",
    "...     print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 4.4062e-04, -2.6241e-01, -1.0192e-01,  ..., -6.2764e-02,\n",
       "           2.7584e-01,  3.7014e-01],\n",
       "         [ 7.2233e-01,  1.6449e-01,  4.0025e-01,  ...,  1.9161e-01,\n",
       "           4.0458e-01, -5.8094e-02],\n",
       "         [ 2.8198e-01, -1.7430e-01,  3.9076e-02,  ...,  2.7681e-02,\n",
       "           1.1886e-01,  9.1439e-01],\n",
       "         ...,\n",
       "         [ 6.8016e-01,  7.9713e-02,  8.3603e-01,  ..., -4.8959e-01,\n",
       "          -2.5017e-01, -2.3518e-01],\n",
       "         [ 3.8105e-02, -8.1751e-01, -3.4076e-01,  ...,  4.4815e-01,\n",
       "           9.6725e-02, -2.0311e-01],\n",
       "         [ 3.5750e-01,  1.9968e-01,  1.7437e-01,  ...,  1.5028e-01,\n",
       "          -2.3665e-01,  5.4390e-02]]], grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"question-answering\")\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp(question=[\"What is extractive question answering?\", \"How do I fine tune?\"],\n",
    "             context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6225805878639221,\n",
       "  'start': 34,\n",
       "  'end': 95,\n",
       "  'answer': 'the task of extracting an answer from a text given a question'},\n",
       " {'score': 0.46403419971466064,\n",
       "  'start': 231,\n",
       "  'end': 254,\n",
       "  'answer': 'a model on a SQuAD task'}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "text = r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ðŸ¤— Transformers? Foo Bar.\",\n",
    "    \"What does ðŸ¤— Transformers provide?\",\n",
    "    \"ðŸ¤— Transformers provides interoperability between which frameworks?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 114])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = tokenizer(text=questions, \n",
    "                text_pair=text, \n",
    "                add_special_tokens=True, \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\")\n",
    "inp.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foo', 'baz')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(['foo', 'bar'], ['baz']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'how',\n",
       " 'many',\n",
       " 'pre',\n",
       " '##train',\n",
       " '##ed',\n",
       " 'models',\n",
       " 'are',\n",
       " 'available',\n",
       " 'in',\n",
       " '[UNK]',\n",
       " 'transformers',\n",
       " '?',\n",
       " 'foo',\n",
       " 'bar',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inp.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'how',\n",
       " 'many',\n",
       " 'pre',\n",
       " '##train',\n",
       " '##ed',\n",
       " 'models',\n",
       " 'are',\n",
       " 'available',\n",
       " 'in',\n",
       " '[UNK]',\n",
       " 'transformers',\n",
       " '?',\n",
       " 'foo',\n",
       " 'bar',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[UNK]',\n",
       " 'transformers',\n",
       " '(',\n",
       " 'formerly',\n",
       " 'known',\n",
       " 'as',\n",
       " 'p',\n",
       " '##yt',\n",
       " '##or',\n",
       " '##ch',\n",
       " '-',\n",
       " 'transformers',\n",
       " 'and',\n",
       " 'p',\n",
       " '##yt',\n",
       " '##or',\n",
       " '##ch',\n",
       " '-',\n",
       " 'pre',\n",
       " '##train',\n",
       " '##ed',\n",
       " '-',\n",
       " 'bert',\n",
       " ')',\n",
       " 'provides',\n",
       " 'general',\n",
       " '-',\n",
       " 'purpose',\n",
       " 'architecture',\n",
       " '##s',\n",
       " '(',\n",
       " 'bert',\n",
       " ',',\n",
       " 'gp',\n",
       " '##t',\n",
       " '-',\n",
       " '2',\n",
       " ',',\n",
       " 'roberta',\n",
       " ',',\n",
       " 'xl',\n",
       " '##m',\n",
       " ',',\n",
       " 'di',\n",
       " '##sti',\n",
       " '##lbert',\n",
       " ',',\n",
       " 'xl',\n",
       " '##net',\n",
       " 'â€¦',\n",
       " ')',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " '(',\n",
       " 'nl',\n",
       " '##u',\n",
       " ')',\n",
       " 'and',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'generation',\n",
       " '(',\n",
       " 'nl',\n",
       " '##g',\n",
       " ')',\n",
       " 'with',\n",
       " 'over',\n",
       " '32',\n",
       " '+',\n",
       " 'pre',\n",
       " '##train',\n",
       " '##ed',\n",
       " 'models',\n",
       " 'in',\n",
       " '100',\n",
       " '+',\n",
       " 'languages',\n",
       " 'and',\n",
       " 'deep',\n",
       " 'inter',\n",
       " '##oper',\n",
       " '##ability',\n",
       " 'between',\n",
       " 'tensor',\n",
       " '##flow',\n",
       " '2',\n",
       " '.',\n",
       " '0',\n",
       " 'and',\n",
       " 'p',\n",
       " '##yt',\n",
       " '##or',\n",
       " '##ch',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = tokenizer(questions[0], text, add_special_tokens=True, padding=True, return_tensors=\"pt\")\n",
    "tokenizer.convert_ids_to_tokens(inp.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 114])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "start = torch.argmax(torch.softmax(out.start_logits, dim=-1), dim=-1).tolist()\n",
    "end = torch.argmax(torch.softmax(out.end_logits, dim=-1), dim=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101,\n",
       "  2129,\n",
       "  2116,\n",
       "  3653,\n",
       "  23654,\n",
       "  2098,\n",
       "  4275,\n",
       "  2024,\n",
       "  2800,\n",
       "  1999,\n",
       "  100,\n",
       "  19081,\n",
       "  1029,\n",
       "  102,\n",
       "  102],\n",
       " [101, 2054, 2515, 100, 19081, 3073, 1029, 102, 100, 102, 0, 0, 0, 0, 0],\n",
       " [101,\n",
       "  100,\n",
       "  19081,\n",
       "  3640,\n",
       "  6970,\n",
       "  25918,\n",
       "  8010,\n",
       "  2090,\n",
       "  2029,\n",
       "  7705,\n",
       "  2015,\n",
       "  1029,\n",
       "  102,\n",
       "  102,\n",
       "  0]]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.input_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: How many pretrained models are available in ðŸ¤— Transformers?\n",
      "span: (11, 11)\n",
      "answer: [19081]\n",
      "\n",
      "question: What does ðŸ¤— Transformers provide?\n",
      "span: (8, 7)\n",
      "answer: []\n",
      "\n",
      "question: ðŸ¤— Transformers provides interoperability between which frameworks?\n",
      "span: (2, 2)\n",
      "answer: [19081]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q, i, s,e in zip(questions, inp.input_ids.tolist(), start,end):\n",
    "    print(f'question: {q}')\n",
    "    print(f'span: {s, e}')\n",
    "    print(f'answer: {i[s:e + 1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in ðŸ¤— Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does ðŸ¤— Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: ðŸ¤— Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "text = r\"\"\"\n",
    "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
    "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
    "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
    "TensorFlow 2.0 and PyTorch.\n",
    "\"\"\"\n",
    "questions = [\n",
    "    \"How many pretrained models are available in ðŸ¤— Transformers?\",\n",
    "    \"What does ðŸ¤— Transformers provide?\",\n",
    "    \"ðŸ¤— Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = tokenizer(questions[0], text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "outputs = model(**i)\n",
    "answer_start = torch.argmax(outputs.start_logits)\n",
    "answer_end = torch.argmax(outputs.end_logits) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2129,  2116,  3653, 23654,  2098,  4275,  2024,  2800,  1999,\n",
       "           100, 19081,  1029,   102,   100, 19081,  1006,  3839,  2124,  2004,\n",
       "          1052, 22123,  2953,  2818,  1011, 19081,  1998,  1052, 22123,  2953,\n",
       "          2818,  1011,  3653, 23654,  2098,  1011, 14324,  1007,  3640,  2236,\n",
       "          1011,  3800,  4294,  2015,  1006, 14324,  1010, 14246,  2102,  1011,\n",
       "          1016,  1010, 23455,  1010, 28712,  2213,  1010,  4487, 16643, 23373,\n",
       "          1010, 28712,  7159,  1529,  1007,  2005,  3019,  2653,  4824,  1006,\n",
       "         17953,  2226,  1007,  1998,  3019,  2653,  4245,  1006, 17953,  2290,\n",
       "          1007,  2007,  2058,  3590,  1009,  3653, 23654,  2098,  4275,  1999,\n",
       "          2531,  1009,  4155,  1998,  2784,  6970, 25918,  8010,  2090, 23435,\n",
       "         12314,  1016,  1012,  1014,  1998,  1052, 22123,  2953,  2818,  1012,\n",
       "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(82), tensor(85))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start, answer_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] how many pretrained models are available in [UNK] transformers? [SEP] [UNK] transformers ( formerly known as pytorch - transformers and pytorch - pretrained - bert ) provides general - purpose architectures ( bert, gpt - 2, roberta, xlm, distilbert, xlnet â€¦ ) for natural language understanding ( nlu ) and natural language generation ( nlg ) with over 32 + pretrained models in 100 + languages and deep interoperability between tensorflow 2. 0 and pytorch. [SEP]'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(i.input_ids.tolist()[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2129,  2116,  3653, 23654,  2098,  4275,  2024,  2800,  1999,\n",
       "           100, 19081,  1029,   102,   100, 19081,  1006,  3839,  2124,  2004,\n",
       "          1052, 22123,  2953,  2818,  1011, 19081,  1998,  1052, 22123,  2953,\n",
       "          2818,  1011,  3653, 23654,  2098,  1011, 14324,  1007,  3640,  2236,\n",
       "          1011,  3800,  4294,  2015,  1006, 14324,  1010, 14246,  2102,  1011,\n",
       "          1016,  1010, 23455,  1010, 28712,  2213,  1010,  4487, 16643, 23373,\n",
       "          1010, 28712,  7159,  1529,  1007,  2005,  3019,  2653,  4824,  1006,\n",
       "         17953,  2226,  1007,  1998,  3019,  2653,  4245,  1006, 17953,  2290,\n",
       "          1007,  2007,  2058,  3590,  1009,  3653, 23654,  2098,  4275,  1999,\n",
       "          2531,  1009,  4155,  1998,  2784,  6970, 25918,  8010,  2090, 23435,\n",
       "         12314,  1016,  1012,  1014,  1998,  1052, 22123,  2953,  2818,  1012,\n",
       "           102]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a812425cb67a4257b17f54704cf7249f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=480.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98d28821afc4d14956ac660123929d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=331070498.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ddc448b14646388b63ea5b498ae8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e317dee8374dde8e5c2e6de673bfbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302ab8a88a1a425598606851007c6994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'The quick brown fox leaps over the hen.',\n",
       "  'score': 0.11760932207107544,\n",
       "  'token': 32564,\n",
       "  'token_str': ' leaps'},\n",
       " {'sequence': 'The quick brown fox watches over the hen.',\n",
       "  'score': 0.09140879660844803,\n",
       "  'token': 11966,\n",
       "  'token_str': ' watches'},\n",
       " {'sequence': 'The quick brown fox jumps over the hen.',\n",
       "  'score': 0.0617658793926239,\n",
       "  'token': 13855,\n",
       "  'token_str': ' jumps'},\n",
       " {'sequence': 'The quick brown fox takes over the hen.',\n",
       "  'score': 0.05134272575378418,\n",
       "  'token': 1239,\n",
       "  'token_str': ' takes'},\n",
       " {'sequence': 'The quick brown fox leapt over the hen.',\n",
       "  'score': 0.028735965490341187,\n",
       "  'token': 34042,\n",
       "  'token_str': ' leapt'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(f\"The quick brown fox {nlp.tokenizer.mask_token} over the hen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamelsmu/transformers/src/transformers/models/auto/modeling_auto.py:790: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4851, 2773, 9711, 18134, 4607]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n",
    "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "token_logits = model(input).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "top_5_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 12120,  2050,  8683,  1181,  3584,  1132,  2964,  1190,  1103,\n",
       "          3584,  1152, 27180,   119,  7993,  1172,  1939,  1104,  1103,  1415,\n",
       "          3827,  1156,  1494,   103,  1412,  6302,  2555, 10988,   119,   102]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(sequence, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 12120,  2050,  8683,  1181,  3584,  1132,  2964,  1190,  1103,\n",
       "          3584,  1152, 27180,   119,  7993,  1172,  1939,  1104,  1103,  1415,\n",
       "          3827,  1156,  1494,   103,  1412,  6302,  2555, 10988,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n",
    "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    "input = tokenizer(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "token_logits = model(input).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "top_5_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casusal Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "model = AutoModelWithLMHead.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "prompt = \"Today the weather is really nice and I am planning on \"\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "prompt_length = len(tokenizer.decode(inputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today the weather is really nice and I am planning on  on it an an an an an an an an an an an an an an an an an an an an an an an an an an an an and an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an a an An an an an an an an an an a an an an an an an an an an an an an an an an an an and an an an An an an an was an an an an an an an an is an an an an an an an an an an an ans an an an an an an An and an an an an be an an an an an an an an an an an a an an an an an an an an an an an an an an an an an ans an An an an an ans an an an, an an an an an an an'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   67,  2840,    19,    18,  1484,    20,   965, 29077,  8719,  1273,\n",
       "            21,    45,   273,    17,    10, 15048,    28, 27511,    21,  4185,\n",
       "            11,    41,  2444,     9,    32,  1025,    20,  8719,    26,    23,\n",
       "           673,   966,    19, 29077, 20643, 27511, 20822, 20643,    19,    17,\n",
       "          6616, 17511,    18,  8978,    20,    18,   777,     9, 19233,  1527,\n",
       "         17669,    19,    24,   673,    17, 28756,   150, 12943,  4354,   153,\n",
       "            27,   442,    37,    45,   668,    21,    24,   256,    20,   416,\n",
       "            22,  2771,  4901,     9, 12943,  4354,   153,    51,    24,  3004,\n",
       "            21, 28142,    23,    65,    20,    18,   416,    34,    24,  2958,\n",
       "         22947,     9,  1177,    45,   668,  3097, 13768,    23,   103,    28,\n",
       "           441,   148,    48, 20522,    19, 12943,  4354,   153, 12860,    34,\n",
       "            18,   326,    27, 17492,   684,    21,  6709,     9,  8585,   123,\n",
       "           266,    19, 12943,  4354,   153,  6872,    24,  3004,    20,    18,\n",
       "          9225,  2198,    19, 12717,   103,    22,   401,    24,  6348,     9,\n",
       "         12943,  4354,   153,  1068,  2768,  2286,    19,    33,   104,    19,\n",
       "           176,    24,  9313,    19, 20086,    28,    45, 10292,     9,     7,\n",
       "             2,  7739,  6122,    23,  3151, 11596,    18,  2194,    27,   343,\n",
       "          2101,    21,    35,   569,  1777,    31,   223,    22,    32,  4744,\n",
       "          2869,     9,    32,   162,    35,    30,    17,    88,    13,   153,\n",
       "          5608,    56,    52,   777,    19,    35,   165,   215,   125,   467,\n",
       "            94,   109,   777,    37,   223,   126,    22,    18,   929,   777,\n",
       "            20,    17,    12,   152, 22857,  9225,  2198,    12,     9,    35,\n",
       "           165,   215,   755,    44,    24,  1730,    22,    18,    17,    12,\n",
       "         22471,    12,  1040,    19,    57,   108,    44,   210,    22,   175,\n",
       "            70,    75,    18,    17,    12, 22471,    12,  1040,    19,    35]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"ner\")\n",
    "sequence = \"\"\"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, \\\n",
    "therefore very close to the Manhattan Bridge which is visible from the window.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'Hu', 'score': 0.999578595161438, 'entity': 'I-ORG', 'index': 1, 'start': 0, 'end': 2}\n",
      "{'word': '##gging', 'score': 0.9909763932228088, 'entity': 'I-ORG', 'index': 2, 'start': 2, 'end': 7}\n",
      "{'word': 'Face', 'score': 0.9982224702835083, 'entity': 'I-ORG', 'index': 3, 'start': 8, 'end': 12}\n",
      "{'word': 'Inc', 'score': 0.9994880557060242, 'entity': 'I-ORG', 'index': 4, 'start': 13, 'end': 16}\n",
      "{'word': 'New', 'score': 0.9994345307350159, 'entity': 'I-LOC', 'index': 11, 'start': 40, 'end': 43}\n",
      "{'word': 'York', 'score': 0.9993196129798889, 'entity': 'I-LOC', 'index': 12, 'start': 44, 'end': 48}\n",
      "{'word': 'City', 'score': 0.9993793964385986, 'entity': 'I-LOC', 'index': 13, 'start': 49, 'end': 53}\n",
      "{'word': 'D', 'score': 0.9862582683563232, 'entity': 'I-LOC', 'index': 19, 'start': 79, 'end': 80}\n",
      "{'word': '##UM', 'score': 0.9514269232749939, 'entity': 'I-LOC', 'index': 20, 'start': 80, 'end': 82}\n",
      "{'word': '##BO', 'score': 0.933659017086029, 'entity': 'I-LOC', 'index': 21, 'start': 82, 'end': 84}\n",
      "{'word': 'Manhattan', 'score': 0.9761654138565063, 'entity': 'I-LOC', 'index': 28, 'start': 114, 'end': 123}\n",
      "{'word': 'Bridge', 'score': 0.9914628863334656, 'entity': 'I-LOC', 'index': 29, 'start': 124, 'end': 130}\n"
     ]
    }
   ],
   "source": [
    "for t in nlp(sequence):\n",
    "    print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
